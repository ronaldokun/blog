{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "pt"
   },
   "source": [
    "# Tensores e Multiplicação de Matrizes\n",
    "> Tensor é a estrutura de dados básica de qualquer biblioteca de Aprendizagem Profunda (i.e. Deep Learning ). O cerne de qualquer biblioteca de Aprendizagem Profunda, seja Tensorflow, Pytorch, Theano etc.., é a manipulação e processamento de Tensores. \n",
    " - toc:true\n",
    " - branch: master\n",
    " - badges: true\n",
    " - hide_binder_badge: true\n",
    " - comments: true\n",
    " - author: Ronaldo S.A. Batista\n",
    " - categories: [pytorch, basics]\n",
    " - image: images/multiplication.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tip: Um número ( escalar ), um vetor, uma matriz ou qualquer vetor n-dimensional são simplesmente Tensores de diferentes ordens. Um escalar é um tensor de ordem (ou dimensão) 0, um vetor é um tensor de ordem 1, uma matriz é um tensor de ordem 2. Uma matriz \"cúbica\" é um tensor de ordem 3 e assim por diante. Assim nos referimos a tudo simplesmente como \"tensores\", sejam eles vetores, matrizes ou objetos de dimensão superior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisamos primeiro importar as dependências, nesse caso a biblioteca pytorch e numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = torch.tensor(5.) ; n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` é uma abreviação de `5.0`. Por padrão todos os tensores criados são do tipo ponto flutuante. Como mencionamos um escalar é simplesmente um tensor de dimensão ou ordem 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quisermos recuperar o tensor escalar como número, útil quando precisamos usar valor fora dos cálculos de tensores, usamos o método `item`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vetor (unidimensional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0., 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) ; x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui criamos um tensor à partir de uma lista, podemos criar tensores de vários objetos python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(range(12)) ; x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pytorch` é fortemente integrado com o `numpy`, inclusive empresta muito da API utilizada por este. Se você possui alguma familiaridade com numpy facilmente consegue compreender e escrever código em Pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0., 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]); x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.from_numpy(x) ; x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mudar o formato do tensor\n",
    "Frequentemente temos que adaptar o formato dos tensores para efetuar diversas operações, isso é feito com o método `reshape`. Anteriormente definimos o tensor unidimensional `x`com 12 elementos. Podemos transformá-lo num tensor bidimensional, desde que mantido o número de elementos: 3x4,  4x3, 2x6 etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se não informarmos 1 dimensão e colocarmos `-1` em seu lugar, o Pytorch infere as demais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(3,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.],\n",
       "         [ 2.,  3.]],\n",
       "\n",
       "        [[ 4.,  5.],\n",
       "         [ 6.,  7.]],\n",
       "\n",
       "        [[ 8.,  9.],\n",
       "         [10., 11.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.reshape(-1, 2, 2) ; x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em vez de criarmos tensores manualmente, podemos usar a função `torch.randn` e criar tensores aleatórios amostrados de uma distribuição normal com média 0 e desvio padrão 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2925,  0.6503, -2.2992,  1.2792],\n",
       "        [ 1.1536,  0.5014,  1.6953, -0.5965],\n",
       "        [ 0.1261, -1.1910, -0.4514, -0.3982]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.randn((3,4)) ; t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0686)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1897)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos criar um tensor Nulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((3,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Unitário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amostragem de uma distribuição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.normal(1,3, (3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9537)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4128)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empilhar Tensores\n",
    "É comum precisarmos combinar diferentes tensores ao longo de algum eixo. Isso normalmente significa criarmos uma dimensão adicional e mantermos as demais dimensões. Sejam 2 tensores 3x4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(12).reshape(3,4).float()\n",
    "y = torch.Tensor([[2,1,4,3], [1,2,3,4], [4,3,2,1]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([[2., 1., 4., 3.],\n",
       "         [1., 2., 3., 4.],\n",
       "         [4., 3., 2., 1.]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos empilhá-los à partir da primeira dimensão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]],\n",
       "\n",
       "        [[ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.stack([x,y], dim=0) ; z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que foi criada uma dimensão adicional e colocado no início, dado o parâmetro `dim=0`. Uma noção geométrica de empilhar os tensores na primeira dimensão seria: somar as linhas dos dois tensores 3x4 e formarmos um tensor 6x4. No entanto não é isso que ocorre, simplesmente criamos um \"cubo\", com a dimensão (2,3,4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.,  3.],\n",
       "         [ 2.,  1.,  4.,  3.]],\n",
       "\n",
       "        [[ 4.,  5.,  6.,  7.],\n",
       "         [ 1.,  2.,  3.,  4.]],\n",
       "\n",
       "        [[ 8.,  9., 10., 11.],\n",
       "         [ 4.,  3.,  2.,  1.]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.stack([x,y], dim=1) ; z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  2.],\n",
       "         [ 1.,  1.],\n",
       "         [ 2.,  4.],\n",
       "         [ 3.,  3.]],\n",
       "\n",
       "        [[ 4.,  1.],\n",
       "         [ 5.,  2.],\n",
       "         [ 6.,  3.],\n",
       "         [ 7.,  4.]],\n",
       "\n",
       "        [[ 8.,  4.],\n",
       "         [ 9.,  3.],\n",
       "         [10.,  2.],\n",
       "         [11.,  1.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.stack([x,y], dim=2) ; z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criar tensores booleanos à partir de comparações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x == y ; z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como em python, ao tentarmos fazer operações numéricas com valores booleanos, temos `True == 1` e `False == 0`. Assim ao somarmos os valores do tensor acima temos o resultado `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "Uma tradução livre para o termo _Broadcasting_ é _transmissão_. Veremos o porquê a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape(3,1)\n",
    "b = torch.arange(2).reshape(1, 2)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como as dimensões não casam, se quisermos adicionar essas matrizes, nós pegamos as dimensões maiores de cada matriz e assim formamos uma matriz 3x2. ( A primeira dimensão maior é 3, da primeira matriz, e a segunda dimensão maior é 2 da segunda matriz.) E assim ajustamos as dimensões das matrizes originais para essa dimensão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a primeira matriz, 3x1, a única coluna é replicada, i.e., a coluna 1 é _transmitida_ para a coluna 2 e a matriz a fica assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.tensor([[0, 0],\n",
    "                  [1, 1],\n",
    "                  [2, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a segunda matrix, 1x2, a primeira linha é replicada 3 vezes, então para o cálculo a matriz b fica assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.tensor([[0,1],\n",
    "                  [0,1],\n",
    "                  [0,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso é feito automaticamente durante a operação efetuada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar se isso está correto e verificar se a operação de soma é equivalente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c + d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O _broadcasting_ em vários casos não é intuitivo, esse foi um exemplo super simples e somente a utilização constante e botar a mão na massa com vários exemplos é que torna o conceito um pouco mais claro. Consulte esse [link](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html) a seguir para um tutorial detalhado sobre broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operações Elemento a Elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor([1,2,4,8])\n",
    "y = torch.Tensor([2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seguintes operações em tensores são efetuadas _elemento a elemento_, isto é, a operação tem como resultado um tensor da mesma forma, porém com a operação efetuada nos elementos de mesmo índice. Caso a forma divirja e as dimensões sejam compatíveis com a operação de broadcasting, este é efetuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y, x-y, x*y, x**y # Exponencial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operação Unária - e.g. Exponenciação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplicação de Matrizes\n",
    "A definição de Multiplicação de 2 matrizes A e B é:\n",
    "![](images/matmul.jpeg \"Credit: https://pt.overleaf.com/articles/matrix-multiplication-revised/qkvxttgrsrqh\")\n",
    "\n",
    "Em código, no geral é mais fácil visualizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a:torch.Tensor,b:torch.Tensor)-> torch.Tensor:\n",
    "    \"Retorna a matrix produto entre a e b\"\n",
    "    # Dimensões\n",
    "    a_linhas, a_cols = a.shape \n",
    "    b_linhas, b_cols = b.shape\n",
    "    \n",
    "    # Verificação de Compatibilidade\n",
    "    assert a_cols==b_linhas\n",
    "    \n",
    "    c = torch.zeros(a_linhas, b_cols)\n",
    "    \n",
    "    for i in range(a_linhas):\n",
    "        for j in range(b_cols):\n",
    "            for k in range(a_cols): # ou b_linhas\n",
    "                c[i,j] += a[i,k] * b[k,j]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 1., 4.],\n",
       "         [3., 1., 2.],\n",
       "         [3., 4., 4.],\n",
       "         [3., 2., 1.]]),\n",
       " torch.Size([4, 3]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O número de colunas da primeira matriz deve ser igual ao número de linhas da segunda para podermos multiplicá-las. Assim vamos usar a função `reshape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(4,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2., 1., 4.],\n",
       "         [3., 1., 2.],\n",
       "         [3., 4., 4.],\n",
       "         [3., 2., 1.]]),\n",
       " torch.Size([4, 3]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos multiplicar ambas as matrizes e teremos como resultado uma matriz `z` de dimensão 3x3. Vamos calcular quanto tempo leva esse cálculo utilizando a definição básica com 3 loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 ms, sys: 6 µs, total: 1.04 ms\n",
      "Wall time: 1.05 ms\n"
     ]
    }
   ],
   "source": [
    "%time z=matmul(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao tentarmos eliminar os \"loops\", nós tornamos a operação mais eficiente, por utilizar debaixo dos panos métodos de \"vetorização\" do Numpy ou Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eliminar o índice `k`, usamos a multiplicação elemento a elemento e o método `sum` do Tensor. Isso efetua a mesma operação efetuada anteriormente com o loop em `k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a:torch.Tensor,b:torch.Tensor)-> torch.Tensor:\n",
    "    \"Retorna a matrix produto entre a e b\"\n",
    "    # Dimensões\n",
    "    a_linhas, a_cols = a.shape \n",
    "    b_linhas, b_cols = b.shape\n",
    "    \n",
    "    # Verificação de Compatibilidade\n",
    "    assert a_cols==b_linhas\n",
    "    \n",
    "    c = torch.zeros(a_linhas, b_cols)\n",
    "    for i in range(a_linhas):\n",
    "        for j in range(b_cols):    \n",
    "            c[i,j] = (a[i,:] * b[:,j]).sum()\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 397 µs, sys: 113 µs, total: 510 µs\n",
      "Wall time: 560 µs\n"
     ]
    }
   ],
   "source": [
    "%time t1=matmul(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para eliminarmos o loop no índice `j`, podemos indexar com o valor especial [None] ou usar `unsqueeze()`  para converter um tensor unidimensional em um vetor de 2 dimensões (A dimensão criada terá valor 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]),\n",
       " torch.Size([1, 3, 4]),\n",
       " torch.Size([1, 3, 4]),\n",
       " torch.Size([3, 1, 4]),\n",
       " torch.Size([3, 4, 1]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x.unsqueeze(0).shape, x[None, :].shape, x.unsqueeze(1).shape, x.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a:torch.Tensor,b:torch.Tensor)-> torch.Tensor:\n",
    "    \"Retorna a matrix produto entre a e b\"\n",
    "    # Dimensões\n",
    "    a_linhas, a_cols = a.shape \n",
    "    b_linhas, b_cols = b.shape\n",
    "    \n",
    "    # Verificação de Compatibilidade\n",
    "    assert a_cols==b_linhas\n",
    "    \n",
    "    c = torch.zeros(a_linhas, b_cols)\n",
    "    for i in range(a_linhas):\n",
    "        c[i] = (a[i].unsqueeze(-1) * b).sum(dim=0)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 174 µs, sys: 50 µs, total: 224 µs\n",
      "Wall time: 230 µs\n"
     ]
    }
   ],
   "source": [
    "%time t1=matmul(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementação do Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76 µs, sys: 22 µs, total: 98 µs\n",
      "Wall time: 102 µs\n"
     ]
    }
   ],
   "source": [
    "%time t1= x.matmul(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forma Equivalente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96 µs, sys: 28 µs, total: 124 µs\n",
      "Wall time: 128 µs\n"
     ]
    }
   ],
   "source": [
    "%time t1= x@y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso foi somente uma pincelada super básica sobre tensores e alguns conceitos e operações básicas sobre eles e uma ilustração em como implementar multiplicação de matrizes de forma básica e mais otimizada  para ilustrar tais conceitos. Por fim simplesmente utilizamos a implementação nativa do Pytorch quando formos de fato utilizar tais operações  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "pt",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "pt",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
