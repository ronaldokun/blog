{
  
    
        "post0": {
            "title": "Tensores e Multiplicação de Matrizes",
            "content": ". Tip: Um número ( escalar ), um vetor, uma matriz ou qualquer vetor n-dimensional são simplesmente Tensores de diferentes ordens. Um escalar é um tensor de ordem (ou dimensão) 0, um vetor é um tensor de ordem 1, uma matriz é um tensor de ordem 2. Uma matriz &quot;cúbica&quot; é um tensor de ordem 3 e assim por diante. Assim nos referimos a tudo simplesmente como &quot;tensores&quot;, sejam eles vetores, matrizes ou objetos de dimensão superior. . Precisamos primeiro importar as dependências, nesse caso a biblioteca pytorch e numpy . import torch import numpy as np . Escalar . n = torch.tensor(5.) ; n . tensor(5.) . 5. é uma abreviação de 5.0. Por padrão todos os tensores criados são do tipo ponto flutuante. Como mencionamos um escalar é simplesmente um tensor de dimensão ou ordem 0. . n.dim() . 0 . Se quisermos recuperar o tensor escalar como número, útil quando precisamos usar valor fora dos cálculos de tensores, usamos o método item: . n.item() . 5.0 . Vetor (unidimensional) . x = torch.tensor([0., 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) ; x . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.]) . Aqui criamos um tensor à partir de uma lista, podemos criar tensores de vários objetos python. . x = torch.tensor(range(12)) ; x . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . x.shape . torch.Size([12]) . x.size() . torch.Size([12]) . x.dim() . 1 . Pytorch é fortemente integrado com o numpy, inclusive empresta muito da API utilizada por este. Se você possui alguma familiaridade com numpy facilmente consegue compreender e escrever código em Pytorch. . x = np.array([0., 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]); x . array([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.]) . x = torch.from_numpy(x) ; x . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.], dtype=torch.float64) . x.numpy() . array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]], dtype=float32) . Mudar o formato do tensor . Frequentemente temos que adaptar o formato dos tensores para efetuar diversas operações, isso é feito com o método reshape. Anteriormente definimos o tensor unidimensional xcom 12 elementos. Podemos transformá-lo num tensor bidimensional, desde que mantido o número de elementos: 3x4, 4x3, 2x6 etc... . x = x.reshape(3,4) . x.dim() . 2 . Se não informarmos 1 dimensão e colocarmos -1 em seu lugar, o Pytorch infere as demais. . x.reshape(3,-1) . tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]], dtype=torch.float64) . x.reshape(-1, 4) . tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]], dtype=torch.float64) . x = x.reshape(-1, 2, 2) ; x . tensor([[[ 0., 1.], [ 2., 3.]], [[ 4., 5.], [ 6., 7.]], [[ 8., 9.], [10., 11.]]], dtype=torch.float64) . x.shape . torch.Size([3, 2, 2]) . Em vez de criarmos tensores manualmente, podemos usar a função torch.randn e criar tensores aleatórios amostrados de uma distribuição normal com média 0 e desvio padrão 1 . t1 = torch.randn((3,4)) ; t1 . tensor([[-1.2925, 0.6503, -2.2992, 1.2792], [ 1.1536, 0.5014, 1.6953, -0.5965], [ 0.1261, -1.1910, -0.4514, -0.3982]]) . t1.mean() . tensor(-0.0686) . t1.std() . tensor(1.1897) . Podemos criar um tensor Nulo . torch.zeros((3,4)) . tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) . Tensor Unitário . torch.ones((2, 3, 4)) . tensor([[[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], [[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]]) . Amostragem de uma distribuição . x = torch.normal(1,3, (3,4)) . x.mean() . tensor(1.9537) . x.std() . tensor(2.4128) . Empilhar Tensores . É comum precisarmos combinar diferentes tensores ao longo de algum eixo. Isso normalmente significa criarmos uma dimensão adicional e mantermos as demais dimensões. Sejam 2 tensores 3x4: . x = torch.arange(12).reshape(3,4).float() y = torch.Tensor([[2,1,4,3], [1,2,3,4], [4,3,2,1]]).float() . x,y . (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]), tensor([[2., 1., 4., 3.], [1., 2., 3., 4.], [4., 3., 2., 1.]])) . Vamos empilhá-los à partir da primeira dimensão . z = torch.stack([x,y], dim=0) ; z . tensor([[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]], [[ 2., 1., 4., 3.], [ 1., 2., 3., 4.], [ 4., 3., 2., 1.]]]) . z.shape . torch.Size([2, 3, 4]) . Vemos que foi criada uma dimensão adicional e colocado no início, dado o parâmetro dim=0. Uma noção geométrica de empilhar os tensores na primeira dimensão seria: somar as linhas dos dois tensores 3x4 e formarmos um tensor 6x4. No entanto não é isso que ocorre, simplesmente criamos um &quot;cubo&quot;, com a dimensão (2,3,4). . z = torch.stack([x,y], dim=1) ; z . tensor([[[ 0., 1., 2., 3.], [ 2., 1., 4., 3.]], [[ 4., 5., 6., 7.], [ 1., 2., 3., 4.]], [[ 8., 9., 10., 11.], [ 4., 3., 2., 1.]]]) . z.shape . torch.Size([3, 2, 4]) . z = torch.stack([x,y], dim=2) ; z . tensor([[[ 0., 2.], [ 1., 1.], [ 2., 4.], [ 3., 3.]], [[ 4., 1.], [ 5., 2.], [ 6., 3.], [ 7., 4.]], [[ 8., 4.], [ 9., 3.], [10., 2.], [11., 1.]]]) . z.shape . torch.Size([3, 4, 2]) . Criar tensores booleanos &#224; partir de compara&#231;&#245;es . z = x == y ; z . tensor([[False, True, False, True], [False, False, False, False], [False, False, False, False]]) . Como em python, ao tentarmos fazer operações numéricas com valores booleanos, temos True == 1 e False == 0. Assim ao somarmos os valores do tensor acima temos o resultado 2. . z.sum() . tensor(2) . Broadcasting . Uma tradução livre para o termo Broadcasting é transmissão. Veremos o porquê a seguir. . a = torch.arange(3).reshape(3,1) b = torch.arange(2).reshape(1, 2) a,b . (tensor([[0], [1], [2]]), tensor([[0, 1]])) . a.shape, b.shape . (torch.Size([3, 1]), torch.Size([1, 2])) . Como as dimensões não casam, se quisermos adicionar essas matrizes, nós pegamos as dimensões maiores de cada matriz e assim formamos uma matriz 3x2. ( A primeira dimensão maior é 3, da primeira matriz, e a segunda dimensão maior é 2 da segunda matriz.) E assim ajustamos as dimensões das matrizes originais para essa dimensão. . Para a primeira matriz, 3x1, a única coluna é replicada, i.e., a coluna 1 é transmitida para a coluna 2 e a matriz a fica assim: . c = torch.tensor([[0, 0], [1, 1], [2, 2]]) . Para a segunda matrix, 1x2, a primeira linha é replicada 3 vezes, então para o cálculo a matriz b fica assim: . d = torch.tensor([[0,1], [0,1], [0,1]]) . Isso é feito automaticamente durante a operação efetuada. . Vamos testar se isso está correto e verificar se a operação de soma é equivalente. . a + b . tensor([[0, 1], [1, 2], [2, 3]]) . c + d . tensor([[0, 1], [1, 2], [2, 3]]) . O broadcasting em vários casos não é intuitivo, esse foi um exemplo super simples e somente a utilização constante e botar a mão na massa com vários exemplos é que torna o conceito um pouco mais claro. Consulte esse link a seguir para um tutorial detalhado sobre broadcasting . Opera&#231;&#245;es Elemento a Elemento . x = torch.Tensor([1,2,4,8]) y = torch.Tensor([2,2,2,2]) . As seguintes operações em tensores são efetuadas elemento a elemento, isto é, a operação tem como resultado um tensor da mesma forma, porém com a operação efetuada nos elementos de mesmo índice. Caso a forma divirja e as dimensões sejam compatíveis com a operação de broadcasting, este é efetuado. . x+y, x-y, x*y, x**y # Exponencial . (tensor([ 3., 4., 6., 10.]), tensor([-1., 0., 2., 6.]), tensor([ 2., 4., 8., 16.]), tensor([ 1., 4., 16., 64.])) . Operação Unária - e.g. Exponenciação . torch.exp(x) . tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03]) . Multiplica&#231;&#227;o de Matrizes . A definição de Multiplicação de 2 matrizes A e B é: . Em código, no geral é mais fácil visualizar: . def matmul(a:torch.Tensor,b:torch.Tensor)-&gt; torch.Tensor: &quot;Retorna a matrix produto entre a e b&quot; # Dimensões a_linhas, a_cols = a.shape b_linhas, b_cols = b.shape # Verificação de Compatibilidade assert a_cols==b_linhas c = torch.zeros(a_linhas, b_cols) for i in range(a_linhas): for j in range(b_cols): for k in range(a_cols): # ou b_linhas c[i,j] += a[i,k] * b[k,j] return c . x, x.shape . (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]), torch.Size([3, 4])) . y, y.shape . (tensor([[2., 1., 4.], [3., 1., 2.], [3., 4., 4.], [3., 2., 1.]]), torch.Size([4, 3])) . O número de colunas da primeira matriz deve ser igual ao número de linhas da segunda para podermos multiplicá-las. Assim vamos usar a função reshape . y = y.reshape(4,3) . y, y.shape . (tensor([[2., 1., 4.], [3., 1., 2.], [3., 4., 4.], [3., 2., 1.]]), torch.Size([4, 3])) . Agora podemos multiplicar ambas as matrizes e teremos como resultado uma matriz z de dimensão 3x3. Vamos calcular quanto tempo leva esse cálculo utilizando a definição básica com 3 loops. . %time z=matmul(x, y) . CPU times: user 1.04 ms, sys: 6 µs, total: 1.04 ms Wall time: 1.05 ms . Ao tentarmos eliminar os &quot;loops&quot;, nós tornamos a operação mais eficiente, por utilizar debaixo dos panos métodos de &quot;vetorização&quot; do Numpy ou Pytorch. . Para eliminar o índice k, usamos a multiplicação elemento a elemento e o método sum do Tensor. Isso efetua a mesma operação efetuada anteriormente com o loop em k . def matmul(a:torch.Tensor,b:torch.Tensor)-&gt; torch.Tensor: &quot;Retorna a matrix produto entre a e b&quot; # Dimensões a_linhas, a_cols = a.shape b_linhas, b_cols = b.shape # Verificação de Compatibilidade assert a_cols==b_linhas c = torch.zeros(a_linhas, b_cols) for i in range(a_linhas): for j in range(b_cols): c[i,j] = (a[i,:] * b[:,j]).sum() return c . %time t1=matmul(x, y) . CPU times: user 397 µs, sys: 113 µs, total: 510 µs Wall time: 560 µs . Para eliminarmos o loop no índice j, podemos indexar com o valor especial [None] ou usar unsqueeze() para converter um tensor unidimensional em um vetor de 2 dimensões (A dimensão criada terá valor 1). . x.shape, x.unsqueeze(0).shape, x[None, :].shape, x.unsqueeze(1).shape, x.unsqueeze(2).shape . (torch.Size([3, 4]), torch.Size([1, 3, 4]), torch.Size([1, 3, 4]), torch.Size([3, 1, 4]), torch.Size([3, 4, 1])) . def matmul(a:torch.Tensor,b:torch.Tensor)-&gt; torch.Tensor: &quot;Retorna a matrix produto entre a e b&quot; # Dimensões a_linhas, a_cols = a.shape b_linhas, b_cols = b.shape # Verificação de Compatibilidade assert a_cols==b_linhas c = torch.zeros(a_linhas, b_cols) for i in range(a_linhas): c[i] = (a[i].unsqueeze(-1) * b).sum(dim=0) return c . %time t1=matmul(x, y) . CPU times: user 174 µs, sys: 50 µs, total: 224 µs Wall time: 230 µs . Implementação do Pytorch . %time t1= x.matmul(y) . CPU times: user 76 µs, sys: 22 µs, total: 98 µs Wall time: 102 µs . Forma Equivalente . %time t1= x@y . CPU times: user 96 µs, sys: 28 µs, total: 124 µs Wall time: 128 µs . Isso foi somente uma pincelada super básica sobre tensores e alguns conceitos e operações básicas sobre eles e uma ilustração em como implementar multiplicação de matrizes de forma básica e mais otimizada para ilustrar tais conceitos. Por fim simplesmente utilizamos a implementação nativa do Pytorch quando formos de fato utilizar tais operações . .",
            "url": "https://habituo.me/2020/05/23/Tensores.html",
            "relUrl": "/2020/05/23/Tensores.html",
            "date": " • May 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://habituo.me/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Sobre mim",
          "content": "Meu nome é Ronaldo S.A. Batista. Formei-me em Ciências Moleculares na USP, mas as tribulações da vida me impediram de seguir o plano de ser um acadêmico. Desde então atuo como fiscal de serviços privados na ANATEL e sou muito grato por tudo que essa posição me proveu. . No entanto, nunca abandonei o estudo de Física, Matemática e Programação, em vez disso os tornei um hobby. . Atualmente programo e faço projetos de Machine Learning nas horas vagas, além de ler e passar o tempo com a família e amigos. . Sempre fui vidrado em como criar melhores hábitos e qual a melhor maneira de atingir nossos objetivos, por que a vida é curta e se pudermos torná-la mais eficiente vale a tentativa. . Esse blog é uma tentativa de criar o hábito de escrever e drenar um pouco a tagarelice constante que rola na minha cachola. .",
          "url": "https://habituo.me/_pages/about.html",
          "relUrl": "/_pages/about.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://habituo.me/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}