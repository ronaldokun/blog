{
  
    
        "post0": {
            "title": "Tensores e Multiplicação de Matrizes",
            "content": ". Tip: Um escalar ( número ), um vetor, uma matriz ou qualquer estrutura n-dimensional são simplesmente tensores de diferentes ordens. Um escalar é um tensor de ordem (ou dimensão) 0, um vetor é um tensor de ordem 1, uma matriz é um tensor de ordem 2. Uma matriz &quot;cúbica&quot; é um tensor de ordem 3 e assim por diante. O Tensor como estrutura de dados do Pytorch é um objeto n-dimensional com algum tipo de escalar - inteiro, real (float), valores lógicos ( booleanos ) - e além disso com algum tipo de dado associado . Desse modo nos referimos a tudo simplesmente como &quot;tensores&quot;, sejam eles vetores, matrizes ou objetos de dimensão superior. . Vamos importar as dependências necessárias, pytorch e numpy . import torch import numpy as np . Criando alguns tensores b&#225;sicos . Para criar um escalar, basta fornecer um número ao construtor. . escalar = torch.tensor(5) escalar . tensor(5) . escalar.dtype . torch.int64 . No entanto o uso de tensores de tipo inteiro é limitado, o padrão é criarmos tensores do tipo ponto flutuante ( float ) e somente o convertermos caso necessário. . escalar = torch.tensor(5.) escalar . tensor(5.) . escalar.dtype . torch.float32 . 5. é uma abreviação de 5.0. . Para checarmos a dimensão do tensor, usamos o método dim . escalar.dim() . 0 . Se quisermos recuperar o tensor escalar como um número python, usamos o método item, este metódo também é válido para tensores não escalares que contém somente 1 elemento: . escalar.item() . 5.0 . x = torch.tensor([[2.]]) x.item() . 2.0 . x = torch.randn(3, 3) ; x . tensor([[ 0.0415, 0.1978, 1.4451], [ 1.1890, -0.7118, 1.6966], [-0.2863, 1.1822, -0.9657]]) . Para criarmos um vetor, basta fornecermos qualquer objeto iterável do python, como lista, tuplas, range, etc... . vetor = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) vetor . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . . Important: Uma restrição importante é que os dados de um tensor devem ter tipo único, não podemos ter um tensor com inteiros, floats e booleanos por exemplo . vetor = torch.tensor([0., 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) vetor . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11.]) . No caso acima fornecemos somente o primeiro número como float: 0. e o Pytorch converteu todos os demais para o tipo float . vetor = torch.tensor([x for x in range(12)]) vetor . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . vetor = torch.tensor(range(12)) vetor . tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . Para checar a dimensão do tensor, poder utilizar o atributo shape ou o método size . vetor.shape . torch.Size([12]) . vetor.size() . torch.Size([12]) . print(f&#39;Dimensão do Vetor: {vetor.dim()}&#39;) . Dimensão do Vetor: 1 . Pytorch é fortemente integrado com o numpy, inclusive empresta muito da API utilizada por este. Se você possui alguma familiaridade com numpy facilmente consegue compreender e escrever código em Pytorch. . Criamos um array com numpy . array = np.array([[0., 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11]]) array . array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) . Para transformarmos em tensor, existem várias formas como passar o array diretamente ao construtor torch.tensor, no entanto o construtor cria um cópia do array original. Outros métodos como: from_numpy e as_tensor compartilham a memória e a modificação efetuada em 1 é refletida no outro. . matrix = torch.from_numpy(array) matrix . tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]], dtype=torch.float64) . matrix[0,0] = -1 . array . array([[-1., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]) . . Note: Para acessar os elementos de um tensor, é usada a mesma indexação já conhecida de objetos python e numpy arrays . Se quisermos recuperar o array novamente, temos o método numpy . vetor.numpy() . array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) . Mudando o formato de um tensor . Frequentemente temos que adaptar o formato dos tensores para efetuar diversas operações, isso é feito com o método reshape. Vamos transformar o tensor unidimensional anteriormente definido com 12 elementos em diferentes formatos. Isso é possível desde que o resultado contenha o mesmo número de elementos. . vetor = vetor.reshape(3,4) vetor . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . vetor.shape . torch.Size([3, 4]) . vetor.dim() . 2 . Se não informarmos 1 dimensão e colocarmos -1 em seu lugar, o Pytorch infere a quantidade de elementos das demais. . vetor = vetor.reshape(3,-1) vetor . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . vetor.reshape(-1, 4) . tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) . vetor = vetor.reshape(-1, 2, 2) vetor . tensor([[[ 0, 1], [ 2, 3]], [[ 4, 5], [ 6, 7]], [[ 8, 9], [10, 11]]]) . vetor.shape . torch.Size([3, 2, 2]) . vetor.dim() . 3 . Se informarmos uma quantidade de elementos divergentes é retornado um RuntimeError . vetor.reshape(3, 4, 4) . RuntimeError Traceback (most recent call last) &lt;ipython-input-32-49750a02386c&gt; in &lt;module&gt; -&gt; 1 vetor.reshape(3, 4, 4) RuntimeError: shape &#39;[3, 4, 4]&#39; is invalid for input of size 12 . Quando não importamos algum dado e precisamos criar tensores específicos, antes de fazê-lo assim manualmente devemos consultar a documentação do Pytorch e utilizar alguma das diversas funções funções disponíveis: . Tensor Nulo . torch.zeros((3,4)) . tensor([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) . Tensor Unitário . torch.ones(3, 4) . tensor([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . Tensor Identidade . torch.eye(3,3) . tensor([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . Tensor com valores amostrados de alguma distribuição . t1 = torch.randn((3,4)) ; t1 . tensor([[ 1.0361, -0.8917, -1.7474, 0.3477], [ 0.9567, 0.0908, 1.1031, 0.6497], [ 1.4214, 0.3836, -0.1230, -0.2525]]) . Como a distribuição normal com média 0 e desvio padrão 1 e tão comum existe esse método para criá-la rapidamente somente fornecendo as dimensões desejadas. . t1.mean() . tensor(0.2479) . t1.std() . tensor(0.9103) . Para outros valores de média e desvio padrão utilize: . x = torch.normal(2,4, (3,4)) . x.mean() . tensor(2.1392) . x.std() . tensor(3.4309) . Para outras distribuições consulte a documentação do Pytorch. . Empilhar Tensores . É comum precisarmos combinar diferentes tensores ao longo de algum eixo. Isso normalmente significa criarmos uma dimensão adicional e mantermos as demais dimensões. Sejam 2 tensores 3x4: . x = torch.arange(12).reshape(3,4).float() y = torch.Tensor([[2,1,4,3], [1,2,3,4], [4,3,2,1]]).float() . x,y . (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]), tensor([[2., 1., 4., 3.], [1., 2., 3., 4.], [4., 3., 2., 1.]])) . Vamos empilhá-los à partir da primeira dimensão . z = torch.stack([x,y], dim=0) ; z . tensor([[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]], [[ 2., 1., 4., 3.], [ 1., 2., 3., 4.], [ 4., 3., 2., 1.]]]) . z.shape . torch.Size([2, 3, 4]) . Vemos que foi criada uma dimensão adicional e colocado no início, dado o parâmetro dim=0. Caso quiséssemos empilhar os tensores numa dimensão distinta. . z = torch.stack([x,y], dim=1) ; z . tensor([[[ 0., 1., 2., 3.], [ 2., 1., 4., 3.]], [[ 4., 5., 6., 7.], [ 1., 2., 3., 4.]], [[ 8., 9., 10., 11.], [ 4., 3., 2., 1.]]]) . z.shape . torch.Size([3, 2, 4]) . z = torch.stack([x,y], dim=2) ; z . tensor([[[ 0., 2.], [ 1., 1.], [ 2., 4.], [ 3., 3.]], [[ 4., 1.], [ 5., 2.], [ 6., 3.], [ 7., 4.]], [[ 8., 4.], [ 9., 3.], [10., 2.], [11., 1.]]]) . z.shape . torch.Size([3, 4, 2]) . Criar tensores booleanos &#224; partir de compara&#231;&#245;es . Ao compararmos dois tensores distintos, o resultado é um tensor de mesma dimensão cujos elementos são os valores booleanos (Verdadeiro ou Falso) da comparação termo a termo . z = x == y z . tensor([[False, True, False, True], [False, False, False, False], [False, False, False, False]]) . Como em python, ao tentarmos fazer operações numéricas com valores booleanos, temos True == 1 e False == 0. Assim ao somarmos os valores do tensor acima temos o resultado 2. . z.sum() . tensor(2) . Opera&#231;&#245;es Elemento a Elemento . As seguintes operações em tensores são efetuadas elemento a elemento, isto é, a operação tem como resultado um tensor da mesma forma, porém com a operação efetuada nos elementos de mesmo índice. Isso normalmente exige que os tensores tenham dimensões iguais. . x = torch.Tensor([1,2,4,8]) y = torch.Tensor([2,2,2,2]) . Adição . x+y . tensor([ 3., 4., 6., 10.]) . Subtração . x-y . tensor([-1., 0., 2., 6.]) . Multiplicação (Elemento a Elemento) . Warning: Não confunda com a multiplicação matricial, esta retorna uma matriz. . x*y . tensor([ 2., 4., 8., 16.]) . Exponencial . x**y . tensor([ 1., 4., 16., 64.]) . Broadcasting . Uma tradução livre para o termo Broadcasting é transmissão. Broadcasting ocorre quando tentamos realizar algumas operações aritméticas, como as operações elemento a elemento vistas no parágrafo anterior, em tensores com dimensões distintas. Se a operação suporta broadcasting e a dimensão dos tensores são compatíveis com algumas regras, essas dimensões são automaticamente expandidas, i.e. os valores das dimensões menores são transmitidos para as dimensões expandidas sem que haja cópia desnecessária dos valores. . As duas regras semânticas básicas que devem ser obedecidas pelos tensores para que seja possível o broadcasting são: . Cada tensor precisa ter ao menos 1 dimensão | Ao alinharmos as dimensões de ambos tensores elas precisar obedecer um dos casos: Serem iguais | Uma delas possuir o valor 1 | Uma delas não existir | | Isso fica mais claro com alguns exemplos. . O caso mais simples de broadcasting é ao multiplicarmos um escalar a um tensor.Seja o exemplo de multiplicação de tensores elemento a elemento.: . a = torch.tensor([1., 2, 3]) b = torch.tensor([2., 2, 2]) a * b . tensor([2., 4., 6.]) . b = 2.0 a * b . tensor([2., 4., 6.]) . O resultado é o mesmo da operação anterior, foi realizado o broadcasting. Vamos ver as regras: . Essa parece ser uma exceção à primeira regra, mesmo um número sendo promovido à um tensor escalar, este por definição possui dimensão nula. No entanto neste caso o escalar é equivalente a um vetor com um único elemento. Para ilustrar o conceito podemos considerar o escalar como um vetor linha - 1x1 | Assim se aplica a regra 2B. Ao alinharmos as colunas do tensor b com o tensor a, a única coluna em b possui a dimensão 1 | . | Conceitualmente o vetor é esticado de 1 para 3 colunas e o valor da coluna 1 em b é transmitido para as outras duas colunas, daí o termo broadcasting | Isso é conceitual somente porque o valor não é duplicado pelo Pytorch, inclusive é mais eficiente quanto à memória do que a operação anterior porque um escalar ocupa menos espaço que um vetor. | . . Um segundo exemplo com uma matriz ( 2 dimensões ) e um vetor ( 1 dimensão ) . a = torch.tensor([[ 0.0, 0.0, 0.0], [10.0, 10.0, 10.0], [20.0, 20.0, 20.0], [30.0, 30.0, 30.0]]) b = torch.tensor([1.0, 2.0, 3.0]) a + b . tensor([[ 1., 2., 3.], [11., 12., 13.], [21., 22., 23.], [31., 32., 33.]]) . Nesse caso temos as dimensões a:4x3 e b:1x3: . A segunda dimensão é igual a 3: 2A | A primeira dimensão de um dos tensores é 1: 2B | . Assim a única linha do vetor b é replicada outras 3 vezes para que o resultado tenha as mesmas dimensão da matriz a . . O broadcasting em vários casos não é intuitivo, esses foram dois exemplos super simples e somente a utilização constante e botar a mão na massa com vários exemplos é que torna o conceito um pouco mais claro. Consulte esse link a seguir para um tutorial detalhado sobre broadcasting . Multiplica&#231;&#227;o de Matrizes . A definição de Multiplicação de 2 matrizes A e B é: . Em código, no geral é mais fácil visualizar: . def matmul(a:torch.Tensor,b:torch.Tensor)-&gt; torch.Tensor: &quot;Retorna a matrix produto entre a e b&quot; # Dimensões a_linhas, a_cols = a.shape b_linhas, b_cols = b.shape # Verificação de Compatibilidade assert a_cols==b_linhas, &quot;O número de colunas da matriz {a_cols} deve ser igual a {b_linhas}&quot; c = torch.zeros(a_linhas, b_cols) for i in range(a_linhas): for j in range(b_cols): for k in range(a_cols): # ou b_linhas c[i,j] += a[i,k] * b[k,j] return c . a = torch.tensor([[1., 2., 3., 4.], [5., 6., 7., 8.], [9., 10., 11., 12]]) b = torch.tensor([[1., 2., 0.], [1., 2., 2.], [2., 2., 0], [1., 0, 1.]]) . Agora podemos multiplicar ambas as matrizes e teremos como resultado uma matriz z de dimensão 3x3. Vamos calcular quanto tempo leva esse cálculo utilizando a definição básica com 3 loops. . %time c=matmul(a, b) . CPU times: user 204 µs, sys: 4.06 ms, total: 4.26 ms Wall time: 4.2 ms . Ao tentarmos eliminar os &quot;loops&quot;, nós tornamos a operação mais eficiente, por utilizar debaixo dos panos métodos de &quot;vetorização&quot;. . Podemos eliminar o índice k, usando a multiplicação elemento a elemento, no lugar do índice k colocamos :, o que significa aplicarmos em todo eixo referenciado. A seguir utilizamos o método sum no eixo. Isso resulta na mesma operação efetuada anteriormente com o loop em k . def matmul(a:torch.Tensor,b:torch.Tensor)-&gt; torch.Tensor: &quot;Retorna a matrix produto entre a e b&quot; # Dimensões a_linhas, a_cols = a.shape b_linhas, b_cols = b.shape # Verificação de Compatibilidade assert a_cols==b_linhas, &quot;O número de colunas da matriz {a_cols} deve ser igual a {b_linhas}&quot; c = torch.zeros(a_linhas, b_cols) for i in range(a_linhas): for j in range(b_cols): c[i,j] = (a[i,:] * b[:,j]).sum() return c . %time c=matmul(a, b) . CPU times: user 1.09 ms, sys: 169 µs, total: 1.25 ms Wall time: 812 µs . Podemos otimizar mais e eliminar o loop no índice j. No entanto não podemos simplesmente eliminá-lo, porque senão viola as regras acima de _ broadcasting: . a[0, :] * b . RuntimeError Traceback (most recent call last) &lt;ipython-input-65-c7015e3e45b9&gt; in &lt;module&gt; -&gt; 1 a[0, :] * b RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1 . a[0, :].shape . torch.Size([4]) . b.shape . torch.Size([4, 3]) . O vetor a indexado possui somente uma dimensão com 4 elementos e b possui 2 dimensões no formato 4x3. Para nos adequar às regras de broadcasting temos que criar um eixo adicional para assim atender à regra 2A: . Para tal podemos indexar o eixo adicional com o valor especial None ou usar o método unsqueeze() . a[0, :, None].shape . torch.Size([4, 1]) . a[0].unsqueeze(-1).shape . torch.Size([4, 1]) . def matmul(a:torch.Tensor,b:torch.Tensor)-&gt; torch.Tensor: &quot;Retorna a matrix produto entre a e b&quot; # Dimensões a_linhas, a_cols = a.shape b_linhas, b_cols = b.shape # Verificação de Compatibilidade assert a_cols==b_linhas c = torch.zeros(a_linhas, b_cols) for i in range(a_linhas): c[i] = (a[i].unsqueeze(-1) * b).sum(dim=0) return c . %time c=matmul(a, b) . CPU times: user 442 µs, sys: 71 µs, total: 513 µs Wall time: 427 µs . Mas isso foi somente para ilustrar os conceitos, na prática usamos a Implementação Otimizada do Pytorch em C++ . %time c= a.matmul(b) . CPU times: user 122 µs, sys: 20 µs, total: 142 µs Wall time: 155 µs . Um método equivalente é usar o operador @ . %time c= a@b . CPU times: user 113 µs, sys: 19 µs, total: 132 µs Wall time: 141 µs . Isso foi somente uma pincelada super básica sobre tensores e alguns conceitos e operações básicas sobre eles e uma ilustração em como implementar multiplicação de matrizes de forma básica e mais otimizada para ilustrar tais conceitos. Por fim simplesmente utilizamos a implementação nativa do Pytorch quando formos de fato utilizar tais operações . . Note: Créditos para a imagem do Postagem: Gayatri Malhotra on Unsplash .",
            "url": "https://habituo.me/pytorch/basics/2020/05/23/Tensores.html",
            "relUrl": "/pytorch/basics/2020/05/23/Tensores.html",
            "date": " • May 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Sobre mim",
          "content": "Meu nome é Ronaldo S.A. Batista. Formei-me em Ciências Moleculares na USP, mas as tribulações da vida me impediram de seguir o plano de ser um acadêmico. Desde então atuo como fiscal de serviços privados na ANATEL e sou muito grato por tudo que essa posição me proveu. . No entanto, nunca abandonei o estudo de Física, Matemática e Programação, em vez disso os tornei um hobby. . Atualmente programo e faço projetos de Machine Learning nas horas vagas, além de ler e passar o tempo com a família e amigos. . Sempre fui vidrado em como criar melhores hábitos e qual a melhor maneira de atingir nossos objetivos, por que a vida é curta e se pudermos torná-la mais eficiente vale a tentativa. . Esse blog é uma tentativa de criar o hábito de escrever e drenar um pouco a tagarelice constante que rola na minha cachola. .",
          "url": "https://habituo.me/_pages/about.html",
          "relUrl": "/_pages/about.html",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://habituo.me/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}